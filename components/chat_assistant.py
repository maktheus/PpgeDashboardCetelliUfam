import streamlit as st
import pandas as pd
import json
import requests
from typing import Dict, List, Any
import time

def render_chat_assistant():
    """
    Render a chat assistant component that allows users to interact with their data
    using a free LLM API
    """
    st.subheader("üí¨ Assistente de Dados")
    st.markdown("Converse com seus dados! Fa√ßa perguntas sobre os indicadores, estat√≠sticas e tend√™ncias.")
    
    # Initialize chat history
    if "chat_messages" not in st.session_state:
        st.session_state.chat_messages = []
    
    # Get current data for context
    from data.data_manager import DataManager
    df = DataManager.get_data()
    
    if df.empty:
        st.warning("Nenhum dado dispon√≠vel. Importe dados primeiro para usar o assistente de chat.")
        return
    
    # Data summary for context
    data_summary = generate_data_summary(df)
    
    # Display chat messages
    chat_container = st.container()
    with chat_container:
        for message in st.session_state.chat_messages:
            with st.chat_message(message["role"]):
                st.markdown(message["content"])
    
    # Chat input
    if prompt := st.chat_input("Fa√ßa uma pergunta sobre seus dados..."):
        # Add user message to chat history
        st.session_state.chat_messages.append({"role": "user", "content": prompt})
        
        # Display user message
        with st.chat_message("user"):
            st.markdown(prompt)
        
        # Generate response
        with st.chat_message("assistant"):
            with st.spinner("Analisando seus dados..."):
                response = generate_llm_response(prompt, data_summary, df)
                st.markdown(response)
        
        # Add assistant response to chat history
        st.session_state.chat_messages.append({"role": "assistant", "content": response})
    
    # Clear chat button
    if st.button("üóëÔ∏è Limpar Conversa"):
        st.session_state.chat_messages = []
        st.rerun()

def generate_data_summary(df: pd.DataFrame) -> Dict[str, Any]:
    """
    Generate a summary of the current dataset for context
    
    Parameters:
    - df: DataFrame containing the data
    
    Returns:
    - summary: Dictionary with data summary information
    """
    summary = {
        "total_records": len(df),
        "columns": list(df.columns),
        "date_range": {},
        "numeric_columns": [],
        "categorical_columns": [],
        "basic_stats": {}
    }
    
    # Identify numeric and categorical columns
    for col in df.columns:
        if pd.api.types.is_numeric_dtype(df[col]):
            summary["numeric_columns"].append(col)
            col_data = df[col].dropna()
            if len(col_data) > 0:
                summary["basic_stats"][col] = {
                    "mean": float(col_data.mean()),
                    "min": float(col_data.min()),
                    "max": float(col_data.max()),
                    "count": int(col_data.count())
                }
            else:
                summary["basic_stats"][col] = {
                    "mean": 0, "min": 0, "max": 0, "count": 0
                }
        else:
            summary["categorical_columns"].append(col)
            if df[col].dtype == 'object':
                unique_vals = df[col].dropna().unique()
                summary["basic_stats"][col] = {
                    "unique_count": len(unique_vals),
                    "top_values": list(unique_vals[:5]) if len(unique_vals) > 0 else []
                }
    
    # Check for date columns
    for col in df.columns:
        if 'date' in col.lower():
            try:
                date_series = pd.to_datetime(df[col], errors='coerce')
                if not date_series.isna().all():
                    summary["date_range"][col] = {
                        "start": str(date_series.min()),
                        "end": str(date_series.max())
                    }
            except:
                pass
    
    return summary

def generate_llm_response(user_question: str, data_summary: Dict[str, Any], df: pd.DataFrame) -> str:
    """
    Generate a response using a free LLM API based on the user's question and data
    
    Parameters:
    - user_question: User's question
    - data_summary: Summary of the current dataset
    - df: DataFrame containing the data
    
    Returns:
    - response: Generated response from the LLM
    """
    try:
        # First, try to answer with local data analysis
        local_response = analyze_question_locally(user_question, data_summary, df)
        
        if local_response:
            return local_response
        
        # If local analysis isn't sufficient, try external LLM
        return call_free_llm_api(user_question, data_summary)
        
    except Exception as e:
        return f"Desculpe, ocorreu um erro ao processar sua pergunta: {str(e)}. Tente reformular sua pergunta ou verifique se os dados est√£o carregados corretamente."

def analyze_question_locally(user_question: str, data_summary: Dict[str, Any], df: pd.DataFrame) -> str:
    """
    Analyze simple questions using local data processing
    
    Parameters:
    - user_question: User's question
    - data_summary: Summary of the current dataset
    - df: DataFrame containing the data
    
    Returns:
    - response: Local analysis response or None if question is too complex
    """
    question_lower = user_question.lower()
    
    # Basic statistics questions
    if any(word in question_lower for word in ['quantos', 'total', 'n√∫mero', 'count']):
        if 'estudantes' in question_lower or 'alunos' in question_lower:
            total_students = len(df)
            return f"Existem **{total_students}** registros de estudantes no dataset atual."
        
        if 'professores' in question_lower or 'docentes' in question_lower:
            if 'advisor_name' in df.columns:
                unique_advisors = df['advisor_name'].nunique()
                return f"Existem **{unique_advisors}** orientadores √∫nicos no dataset."
        
        if 'programas' in question_lower:
            if 'program' in df.columns:
                unique_programs = df['program'].nunique()
                programs_list = df['program'].unique().tolist()
                return f"Existem **{unique_programs}** programas: {', '.join(programs_list)}"
    
    # Average/mean questions
    if any(word in question_lower for word in ['m√©dia', 'average', 'mean']):
        if 'tempo' in question_lower and 'defesa' in question_lower:
            if 'time_to_defense_days' in df.columns:
                avg_time = df['time_to_defense_days'].mean()
                if not pd.isna(avg_time):
                    return f"O tempo m√©dio para defesa √© de **{avg_time:.1f} dias** (aproximadamente {avg_time/365:.1f} anos)."
        
        if 'publica√ß√µes' in question_lower or 'publications' in question_lower:
            if 'publications' in df.columns:
                avg_pubs = df['publications'].mean()
                if not pd.isna(avg_pubs):
                    return f"O n√∫mero m√©dio de publica√ß√µes por estudante √© **{avg_pubs:.1f}**."
    
    # Range/period questions
    if any(word in question_lower for word in ['per√≠odo', 'range', 'anos']):
        date_info = []
        for col, date_range in data_summary.get("date_range", {}).items():
            date_info.append(f"**{col}**: {date_range['start']} at√© {date_range['end']}")
        
        if date_info:
            return f"Per√≠odos dos dados:\n" + "\n".join(date_info)
    
    # Column information
    if any(word in question_lower for word in ['colunas', 'campos', 'dados dispon√≠veis']):
        columns = data_summary.get("columns", [])
        return f"**Dados dispon√≠veis ({len(columns)} campos):**\n" + "\n".join([f"‚Ä¢ {col}" for col in columns])
    
    return None

def call_free_llm_api(user_question: str, data_summary: Dict[str, Any]) -> str:
    """
    Call a free LLM API to generate a response
    
    Parameters:
    - user_question: User's question
    - data_summary: Summary of the current dataset
    
    Returns:
    - response: Response from the LLM API
    """
    # Try multiple free API options
    
    # Option 1: Try Hugging Face if API key is available
    try:
        if hasattr(st, 'secrets') and 'HUGGINGFACE_API_KEY' in st.secrets:
            return call_huggingface_api(user_question, data_summary, st.secrets["HUGGINGFACE_API_KEY"])
    except:
        pass
    
    # Option 2: Try using a free public endpoint (no API key required)
    try:
        return call_free_public_llm(user_question, data_summary)
    except:
        pass
    
    # Fallback: Provide information about setting up API access
    return """
    Para usar o assistente avan√ßado de IA, voc√™ pode configurar uma das seguintes op√ß√µes gratuitas:
    
    **Op√ß√£o 1 - Hugging Face (Recomendado):**
    1. Acesse https://huggingface.co/
    2. Crie uma conta gratuita
    3. V√° em Settings > Access Tokens
    4. Crie um novo token
    5. Configure HUGGINGFACE_API_KEY nos secrets do Streamlit
    
    **Op√ß√£o 2 - OpenAI (Gratuito com limita√ß√µes):**
    1. Acesse https://platform.openai.com/
    2. Crie uma conta e obtenha cr√©ditos gratuitos
    3. Configure OPENAI_API_KEY nos secrets
    
    **Enquanto isso, posso responder perguntas b√°sicas sobre seus dados usando an√°lise local.**
    Exemplos: "Quantos estudantes temos?", "Qual a m√©dia de tempo para defesa?", "Que dados est√£o dispon√≠veis?"
    """

def call_huggingface_api(user_question: str, data_summary: Dict[str, Any], api_key: str) -> str:
    """Call Hugging Face API with the provided API key"""
    try:
        # Prepare context
        context = f"""
        Contexto dos dados:
        - Total de registros: {data_summary['total_records']}
        - Colunas dispon√≠veis: {', '.join(data_summary['columns'])}
        - Colunas num√©ricas: {', '.join(data_summary['numeric_columns'])}
        - Colunas categ√≥ricas: {', '.join(data_summary['categorical_columns'])}
        """
        
        # Prepare prompt
        prompt = f"""
        Voc√™ √© um assistente especializado em an√°lise de dados acad√™micos de programas de p√≥s-gradua√ß√£o.
        
        {context}
        
        Pergunta do usu√°rio: {user_question}
        
        Responda de forma clara e objetiva em portugu√™s, focando nos dados dispon√≠veis.
        """
        
        # Call Hugging Face API
        API_URL = "https://api-inference.huggingface.co/models/microsoft/DialoGPT-medium"
        headers = {"Authorization": f"Bearer {api_key}"}
        
        payload = {
            "inputs": prompt,
            "parameters": {
                "max_length": 500,
                "temperature": 0.7,
                "return_full_text": False
            }
        }
        
        response = requests.post(API_URL, headers=headers, json=payload, timeout=30)
        
        if response.status_code == 200:
            result = response.json()
            if isinstance(result, list) and len(result) > 0:
                return result[0].get("generated_text", "Desculpe, n√£o consegui gerar uma resposta adequada.")
            else:
                return "Desculpe, n√£o consegui gerar uma resposta adequada."
        else:
            return f"Erro na API: {response.status_code}. Tente novamente em alguns momentos."
            
    except requests.exceptions.Timeout:
        return "A consulta est√° demorando muito. Tente fazer uma pergunta mais espec√≠fica."
    except Exception as e:
        return f"Erro ao conectar com o servi√ßo de IA: {str(e)}"

def call_free_public_llm(user_question: str, data_summary: Dict[str, Any]) -> str:
    """Try to call a free public LLM endpoint (no API key required)"""
    # For now, this will use enhanced local analysis
    # In the future, this could connect to other free services
    return generate_enhanced_local_response(user_question, data_summary)

def generate_enhanced_local_response(user_question: str, data_summary: Dict[str, Any]) -> str:
    """Generate an enhanced response using local analysis"""
    question_lower = user_question.lower()
    
    # More sophisticated pattern matching
    response_parts = []
    
    # Check for data overview questions
    if any(word in question_lower for word in ['vis√£o geral', 'overview', 'resumo', 'summary']):
        response_parts.append(f"**Vis√£o Geral dos Dados:**")
        response_parts.append(f"‚Ä¢ Total de registros: {data_summary['total_records']}")
        response_parts.append(f"‚Ä¢ Campos dispon√≠veis: {len(data_summary['columns'])}")
        response_parts.append(f"‚Ä¢ Dados num√©ricos: {len(data_summary['numeric_columns'])} campos")
        response_parts.append(f"‚Ä¢ Dados categ√≥ricos: {len(data_summary['categorical_columns'])} campos")
    
    # Check for statistical questions
    if any(word in question_lower for word in ['estat√≠sticas', 'statistics', 'n√∫meros', 'dados']):
        if data_summary['basic_stats']:
            response_parts.append("**Estat√≠sticas Principais:**")
            for col, stats in data_summary['basic_stats'].items():
                if col in data_summary['numeric_columns']:
                    response_parts.append(f"‚Ä¢ {col}: m√©dia={stats.get('mean', 0):.2f}, min={stats.get('min', 0)}, max={stats.get('max', 0)}")
                elif 'unique_count' in stats:
                    response_parts.append(f"‚Ä¢ {col}: {stats['unique_count']} valores √∫nicos")
    
    # Check for date range questions
    if any(word in question_lower for word in ['per√≠odo', 'range', 'datas', 'tempo']):
        if data_summary.get('date_range'):
            response_parts.append("**Per√≠odos dos Dados:**")
            for col, date_range in data_summary['date_range'].items():
                response_parts.append(f"‚Ä¢ {col}: {date_range['start']} at√© {date_range['end']}")
    
    if response_parts:
        return "\n".join(response_parts)
    
    # Fallback to suggesting specific questions
    return """
    Posso ajud√°-lo a analisar seus dados! Aqui est√£o algumas perguntas que posso responder:
    
    **Informa√ß√µes b√°sicas:**
    ‚Ä¢ "Quantos registros temos?"
    ‚Ä¢ "Que dados est√£o dispon√≠veis?"
    ‚Ä¢ "Qual √© o per√≠odo dos dados?"
    
    **An√°lises espec√≠ficas:**
    ‚Ä¢ "Quantos estudantes temos?"
    ‚Ä¢ "Qual a m√©dia de tempo para defesa?"
    ‚Ä¢ "Quantos orientadores √∫nicos existem?"
    
    Fa√ßa uma pergunta espec√≠fica sobre seus dados!
    """

def render_chat_help():
    """
    Render help section for the chat assistant
    """
    with st.expander("üí° Como usar o Assistente de Dados"):
        st.markdown("""
        **Exemplos de perguntas que voc√™ pode fazer:**
        
        üìä **Estat√≠sticas b√°sicas:**
        - "Quantos estudantes temos no total?"
        - "Qual √© a m√©dia de tempo para defesa?"
        - "Quantas publica√ß√µes os estudantes t√™m em m√©dia?"
        
        üìà **Tend√™ncias e compara√ß√µes:**
        - "Qual programa tem melhor taxa de conclus√£o?"
        - "Como est√° evoluindo o n√∫mero de defesas ao longo dos anos?"
        - "Quais s√£o os orientadores mais produtivos?"
        
        üîç **Informa√ß√µes dos dados:**
        - "Que dados est√£o dispon√≠veis?"
        - "Qual √© o per√≠odo coberto pelos dados?"
        - "Quantos programas diferentes temos?"
        
        **Dicas:**
        - Seja espec√≠fico em suas perguntas
        - Use termos relacionados aos seus dados
        - Se n√£o entender a resposta, reformule a pergunta
        """)